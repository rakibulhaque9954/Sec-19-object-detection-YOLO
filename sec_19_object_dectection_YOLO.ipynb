{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uDULcwEwPILC"
      ],
      "mount_file_id": "1pkst5EQvG1bLIJMZ0qzSUXX3dzmljDFi",
      "authorship_tag": "ABX9TyP28GFrMEODcM2x29RnWpWR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakibulhaque9954/Sec-19-object-detection-YOLO/blob/main/sec_19_object_dectection_YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "7d0HkHsGlTCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "7g9Bj1m_kWOf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import seaborn as sns### visualizations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "from datetime import datetime\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape, LeakyReLU)\n",
        "from keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.regularizers import L2, L1\n",
        "from keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Download\n"
      ],
      "metadata": {
        "id": "xLPJaaQIlWk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d huanghanchina/pascal-voc-2012"
      ],
      "metadata": {
        "id": "WfONqnnylWBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88700bdf-6c94-4171-bfec-956d715ab0ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading pascal-voc-2012.zip to /content\n",
            " 99% 3.61G/3.63G [00:43<00:00, 139MB/s]\n",
            "100% 3.63G/3.63G [00:43<00:00, 90.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_list=['2007_000027.jpg','2007_000032.jpg','2007_000033.jpg','2007_000039.jpg','2007_000042.jpg','2007_000061.jpg',\n",
        "          '2007_000063.jpg','2007_000068.jpg','2007_000121.jpg','2007_000123.jpg','2007_000129.jpg','2007_000170.jpg',\n",
        "          '2007_000175.jpg','2007_000187.jpg','2007_000241.jpg','2007_000243.jpg','2007_000250.jpg','2007_000256.jpg',\n",
        "          '2007_000272.jpg','2007_000323.jpg','2007_000332.jpg','2007_000333.jpg','2007_000346.jpg','2007_000363.jpg',\n",
        "          '2007_000364.jpg','2007_000392.jpg','2007_000423.jpg','2007_000452.jpg','2007_000464.jpg','2007_000480.jpg',\n",
        "          '2007_000491.jpg','2007_000504.jpg','2007_000515.jpg','2007_000528.jpg','2007_000529.jpg','2007_000549.jpg',\n",
        "          '2007_000559.jpg','2007_000572.jpg','2007_000584.jpg','2007_000629.jpg','2007_000636.jpg','2007_000645.jpg',\n",
        "          '2007_000648.jpg','2007_000661.jpg','2007_000663.jpg','2007_000664.jpg','2007_000676.jpg','2007_000713.jpg',\n",
        "          '2007_000720.jpg','2007_000727.jpg','2007_000733.jpg','2007_000738.jpg','2007_000762.jpg','2007_000768.jpg',\n",
        "          '2007_000783.jpg','2007_000793.jpg','2007_000799.jpg','2007_000804.jpg','2007_000807.jpg','2007_000822.jpg',\n",
        "          '2007_001299.jpg','2007_001311.jpg','2007_001321.jpg','2007_001340.jpg']"
      ],
      "metadata": {
        "id": "zh9T6N-pM1F8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/pascal-voc-2012.zip' -d '/content/dataset'"
      ],
      "metadata": {
        "id": "j7_oTnYkmAwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "R7_xjaszmkZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = '/content/dataset/VOC2012/JPEGImages/'\n",
        "train_maps = '/content/dataset/VOC2012/Annotations/'\n",
        "\n",
        "\n",
        "val_images='/content/dataset/VOC2012/ValJPEGImages/'\n",
        "val_maps='/content/dataset/VOC2012/ValAnnotations/'\n",
        "\n",
        "classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable',\n",
        "         'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
        "\n",
        "B = 2 # b is the number of bounding box according to the YOLO paper\n",
        "N_CLASSES = len(classes)\n",
        "H, W = 224, 224\n",
        "SPLIT_SIZE = H//32\n",
        "N_EPOCHS = 100\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "5iAisJA4mVuy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SPLIT_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulLbv6jkErke",
        "outputId": "379ec155-9521-4b5f-e1de-8e9c295832e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making val dataset\n",
        "!mkdir /content/dataset/VOC2012/ValJPEGImages/\n",
        "!mkdir /content/dataset/VOC2012/ValAnnotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiuRUDSdL5eD",
        "outputId": "f5d8a28a-e7c9-4c31-b87d-284fcd671f86"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset/VOC2012/ValJPEGImages/’: File exists\n",
            "mkdir: cannot create directory ‘/content/dataset/VOC2012/ValAnnotations/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in val_list:\n",
        "  shutil.move(train_maps+name[:-3]+\"xml\", val_maps+name[:-3]+\"xml\")"
      ],
      "metadata": {
        "id": "8xb0M7jqMgR-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in val_list:\n",
        "  shutil.move(train_images+name, val_images+name)"
      ],
      "metadata": {
        "id": "-UgsUkZ3Mh_j"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "tN8I28mLElBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_xml(filename):\n",
        "  tree = ET.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  size_tree = root.find('size')\n",
        "  height = float(size_tree.find('height').text)\n",
        "  width = float(size_tree.find('width').text)\n",
        "  bounding_boxes=[]\n",
        "  for object_tree in root.findall('object'):\n",
        "    for bounding_box in object_tree.iter('bndbox'):\n",
        "      xmin = (float(bounding_box.find('xmin').text))\n",
        "      ymin = (float(bounding_box.find('ymin').text))\n",
        "      xmax = (float(bounding_box.find('xmax').text))\n",
        "      ymax = (float(bounding_box.find('ymax').text))\n",
        "      break\n",
        "    class_name = object_tree.find('name').text\n",
        "    class_dict={classes[i]:i for i in range(len(classes))} # converting all classes to int\n",
        "    bounding_box = [\n",
        "        (xmin+xmax)/(2*width),(ymin+ymax)/(2*height),(xmax-xmin)/width,\n",
        "        (ymax-ymin)/height,class_dict[class_name]]\n",
        "         # according to the formula x_centre, y_centre, width, height\n",
        "    bounding_boxes.append(bounding_box)\n",
        "  return tf.convert_to_tensor(bounding_boxes)"
      ],
      "metadata": {
        "id": "5BoOct7nSLJv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each annotation xml file contains details about each image which relay the following details:\n",
        "- The image's filename\n",
        "- The size of the image(height and width)\n",
        "- BB information and Class label\n",
        "\n",
        "They are crucial for training along with the images"
      ],
      "metadata": {
        "id": "rbvQtm2kp2Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_xml(train_maps+'2007_000830.xml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pftRlydqPph",
        "outputId": "0b1c21ef-a231-401a-d96b-c716347fb2a1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
              "array([[ 0.476     ,  0.736     ,  0.456     ,  0.50133336, 10.        ],\n",
              "       [ 0.409     ,  0.79466665,  0.218     ,  0.41066667,  8.        ],\n",
              "       [ 0.6       ,  0.73066664,  0.192     ,  0.38933334,  8.        ],\n",
              "       [ 0.568     ,  0.564     ,  0.136     ,  0.19466667,  8.        ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(bounding_boxes):\n",
        "  output_label=np.zeros((SPLIT_SIZE,SPLIT_SIZE,N_CLASSES+5))\n",
        "  for b in range(len(bounding_boxes)):\n",
        "    grid_x=bounding_boxes[...,b,0]*SPLIT_SIZE # x_centre * patch_size\n",
        "    grid_y=bounding_boxes[...,b,1]*SPLIT_SIZE # y_centre * patch_size\n",
        "    i=int(grid_x) # rounding to find the index where the object is located\n",
        "    j=int(grid_y)\n",
        "\n",
        "    output_label[i,j,0:5]=[1.,grid_x%1,grid_y%1,bounding_boxes[...,b,2],bounding_boxes[...,b,3]]\n",
        "    # assigning the values like the formula [confidence(objectness), x_centre_grid_cell, y_centre_grid_cell, width , height]\n",
        "    output_label[i,j,5+int(bounding_boxes[...,b,4])]=1. # ... is for batch compuatations same as all teh elements\n",
        "    # this is just changing the class index to categorical mode like assigning a 1 to where the class is in the classes list,\n",
        "    # for ex: class 14 [1, 2, 3...14, ...20] to class [0, 0, 0...., 1, 0, 0, 0]\n",
        "\n",
        "  return tf.convert_to_tensor(output_label,tf.float32)"
      ],
      "metadata": {
        "id": "Dl2dEYpVLFb7"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second part of the equation where this time the location is caluclated with precision using grid cells by dividing the images into patches for instance, 7x7 patches and the normalizing the values, the part only where the objects medial point is found in the grid cells are placed with a value of 1, while other cells are 0."
      ],
      "metadata": {
        "id": "TkGZl87t7a8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([[[1, 2, 3, 4, 5],\n",
        "                 [0, 0, 0, 0, 0]],\n",
        "                 [[6, 7, 8, 9, 10],\n",
        "                 [0, 0, 0, 0, 0]]])"
      ],
      "metadata": {
        "id": "w4cveVsf75qp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[0, 1, 0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWKYKGe--W89",
        "outputId": "a179006b-9187-49cd-b867-ddf5f5679002"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im_paths=[]\n",
        "xml_paths=[]\n",
        "\n",
        "val_im_paths=[]\n",
        "val_xml_paths=[]\n",
        "\n",
        "\n",
        "for i in os.listdir(train_maps):\n",
        "\n",
        "  im_paths.append(train_images+i[:-3]+'jpg')\n",
        "  xml_paths.append(train_maps+i)\n",
        "\n",
        "for i in os.listdir(val_maps):\n",
        "\n",
        "  val_im_paths.append(val_images+i[:-3]+'jpg')\n",
        "  val_xml_paths.append(val_maps+i)\n",
        "\n",
        "print(len(im_paths),len(xml_paths))\n",
        "print(len(val_im_paths),len(val_xml_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDLm5jQaBNgD",
        "outputId": "372a8f59-f289-417c-97b2-7c78773d5144"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17061 17061\n",
            "64 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Val dataset"
      ],
      "metadata": {
        "id": "uDULcwEwPILC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((im_paths, xml_paths))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_im_paths, val_xml_paths))"
      ],
      "metadata": {
        "id": "f2IKqukNPHTU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bMAiuDbRawD",
        "outputId": "f87a154e-7bd4-40fd-c723-cabdf4a09eb7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'/content/dataset/VOC2012/ValJPEGImages/2007_000032.jpg'>, <tf.Tensor: shape=(), dtype=string, numpy=b'/content/dataset/VOC2012/ValAnnotations/2007_000032.xml'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_imboxes(im_path, xml_path):\n",
        "\n",
        "  img = tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
        "  img = tf.cast(tf.image.resize(img, size=[H, W]), dtype=tf.float32)\n",
        "\n",
        "\n",
        "  bboxes = tf.numpy_function(func=preprocess_xml, inp=[xml_path], Tout=tf.float32)\n",
        "  return img, bboxes"
      ],
      "metadata": {
        "id": "IIkCUbrYSnBL"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping\n",
        "train_dataset = train_dataset.map(get_imboxes)\n",
        "val_dataset = val_dataset.map(get_imboxes)"
      ],
      "metadata": {
        "id": "2U6FrOHhTu3D"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in train_dataset.skip(20):\n",
        "  print(i.shape, j)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnZkO_iST-Gh",
        "outputId": "e0f58462-5d14-4d10-cd11-4c694e8fe4e7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) tf.Tensor(\n",
            "[[ 0.275       0.5615616   0.414       0.8768769  14.        ]\n",
            " [ 0.304       0.07507508  0.156       0.14414415 14.        ]\n",
            " [ 0.582       0.5705706   0.508       0.8588589  14.        ]], shape=(3, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('out_1.jpg', np.array(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAD53yI8UGcB",
        "outputId": "f416cb30-c169-486a-fe75-fa6d6ce7d2b9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes[14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mBdQ8ATBUfjt",
        "outputId": "b3ffb887-5699-4d0a-f39f-32fa0dde6582"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'person'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation"
      ],
      "metadata": {
        "id": "-EJmbt5QCQtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = A.Compose([\n",
        "    A.Resize(H,W),\n",
        "    A.RandomCrop(\n",
        "         width=np.random.randint(int(0.9*W),W),\n",
        "         height=np.random.randint(int(0.9*H),H), p=0.5),\n",
        "    A.RandomScale(scale_limit=0.1, interpolation=cv2.INTER_LANCZOS4,p=0.5),\n",
        "    A.HorizontalFlip(p=0.5,),\n",
        "    A.Resize(H,W),\n",
        "\n",
        "], bbox_params=A.BboxParams(format='yolo', ))"
      ],
      "metadata": {
        "id": "tqTrNRq26PAe"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aug_albument(image,bboxes):\n",
        "  augmented=transforms(image=image,bboxes=bboxes)\n",
        "  return [tf.convert_to_tensor(augmented[\"image\"],dtype=tf.float32),\n",
        "          tf.convert_to_tensor(augmented[\"bboxes\"],dtype=tf.float32)]"
      ],
      "metadata": {
        "id": "B1OrWDmy6SG7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(image,bboxes):\n",
        "    aug= tf.numpy_function(func=aug_albument, inp=[image,bboxes], Tout=(tf.float32,tf.float32))\n",
        "    return aug[0],aug[1]"
      ],
      "metadata": {
        "id": "heugj8M_6Ukd"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=train_dataset.map(process_data)"
      ],
      "metadata": {
        "id": "WUPAiYzF6Wx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in train_dataset.skip(2):\n",
        "  print(i.shape,j)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPmEF2q26Zwp",
        "outputId": "8a74af77-c321-46a8-bead-94e26928140a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) tf.Tensor([[0.501     0.5014006 0.998     0.9971989 7.       ]], shape=(1, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('out_2.jpg',np.array(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jBPLhZd6c-e",
        "outputId": "6c007cdd-bd49-4c7e-eab7-74f9108136c5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_augment(img,y):\n",
        "  img = tf.image.random_brightness(img, max_delta=50.)\n",
        "  img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
        "  img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
        "  #img = tf.image.random_hue(img, max_delta=0.5 )\n",
        "  img = tf.clip_by_value(img, 0, 255)\n",
        "  labels=tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
        "  return img,labels"
      ],
      "metadata": {
        "id": "HsfbGHhpCGBl"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img,y):\n",
        "  img = tf.cast(tf.image.resize(img, size=[H, W]), dtype=tf.float32)\n",
        "\n",
        "  labels=tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
        "  return img,labels"
      ],
      "metadata": {
        "id": "duxGcgsXCIdC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=train_dataset.map(preprocess_augment)\n",
        "val_dataset=val_dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "AyS-mARiCLUW"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalised Datasets"
      ],
      "metadata": {
        "id": "kGi1-CreCV3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=(\n",
        "  train_dataset.\n",
        "  batch(BATCH_SIZE).\n",
        "  prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "9je26Kk28Veh"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = (\n",
        "    val_dataset.\n",
        "    batch(BATCH_SIZE).\n",
        "    prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "QqVSfFBc8XDx"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in train_dataset.take(1):\n",
        "  print(i.shape,j)\n",
        "  break"
      ],
      "metadata": {
        "id": "dDavfCgO8dYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('out_3.jpg',np.array(i[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u3Zhy3RCecC",
        "outputId": "5c417fd4-5c23-42e9-d4cc-df582e68be3a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelling"
      ],
      "metadata": {
        "id": "CaYRfASl-Y-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO Resnet and EfficientNetB1"
      ],
      "metadata": {
        "id": "3bN4suHE-a-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FILTERS = 512\n",
        "OUTPUT_DIM = N_CLASSES + 5 * B # according to the paper the total should be 30\n",
        "# Total_classes + 5(objectness, bb(x, y, w, h)) + B(2)"
      ],
      "metadata": {
        "id": "JHddtZuM-aW1"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_model = tf.keras.applications.resnet50.ResNet50(\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB1(\n",
        "    weights='imagenet',\n",
        "    input_shape=(H, W, 3),\n",
        "    include_top=False\n",
        ")\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "ebQSvW6ABUak"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Feature extractor\n",
        "    base_model,\n",
        "\n",
        "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    # he_normal helps with the vanishing gradient problem during back propogation\n",
        "\n",
        "\n",
        "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "\n",
        "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "\n",
        "    Conv2D(NUM_FILTERS, (3, 3), padding = 'same', kernel_initializer='he_normal'),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(NUM_FILTERS, kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(SPLIT_SIZE * SPLIT_SIZE * OUTPUT_DIM, activation='sigmoid'),\n",
        "\n",
        "    Reshape((SPLIT_SIZE, SPLIT_SIZE, OUTPUT_DIM))\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRVdVRkmBrmi",
        "outputId": "de1b4dc9-c7b9-41e9-b55f-8c42636b9e04"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb1 (Functional  (None, 7, 7, 1280)        6575239   \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 7, 7, 512)         5898752   \n",
            "                                                                 \n",
            " batch_normalization_24 (Ba  (None, 7, 7, 512)         2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_30 (LeakyReLU)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_25 (Ba  (None, 7, 7, 512)         2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_31 (LeakyReLU)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_26 (Ba  (None, 7, 7, 512)         2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_32 (LeakyReLU)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " leaky_re_lu_33 (LeakyReLU)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_27 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_34 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1470)              754110    \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 7, 7, 30)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33161285 (126.50 MB)\n",
            "Trainable params: 26581950 (101.40 MB)\n",
            "Non-trainable params: 6579335 (25.10 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "BEknQXDXFyUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint"
      ],
      "metadata": {
        "id": "ZuhX9gcTF0gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/YOLO-Model/yolo_resnet_50.h5'\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "BcvIwrOoFPlo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LR schedular"
      ],
      "metadata": {
        "id": "Fgjd5M58F5CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def schedular(epoch, lr):\n",
        "  if epoch < 40:\n",
        "    return 1e-3\n",
        "  elif 50 >= epoch < 80:\n",
        "    return 5e-4\n",
        "  else:\n",
        "    return 1e-4"
      ],
      "metadata": {
        "id": "4AZ2NShuIwWe"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(schedular)"
      ],
      "metadata": {
        "id": "ydjjbg9qJMcu"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO Loss"
      ],
      "metadata": {
        "id": "BI-8f5BEF81a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
        "                         boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
        "                         boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
        "                         boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
        "                        axis=-1)\n",
        "                        # calculation of xmin, ymin, xmax, ymax of box1\n",
        "\n",
        "    boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
        "                         boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
        "                         boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
        "                         boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
        "                        axis=-1)\n",
        "                        # calculation of xmin, ymin, xmax, ymax of box2\n",
        "\n",
        "    lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
        "    rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
        "    # calculation of xmin, ymin, xmax, ymax of the intersection of the box1 and box2\n",
        "\n",
        "    intersection = tf.maximum(0.0, rd - lu) # calculation of width and height of the intersection box to calculate the area\n",
        "    inter_square = intersection[..., 0] * intersection[..., 1] # area calculation\n",
        "\n",
        "    square1 = boxes1[..., 2] * boxes1[..., 3] # calculation of area of box1\n",
        "    square2 = boxes2[..., 2] * boxes2[..., 3] # calculation of area of box2\n",
        "\n",
        "    union_square = tf.maximum(square1 + square2 - inter_square, 1e-10) # calculating the intersection/ union area\n",
        "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0) # returns IOU value which Intersection/Union"
      ],
      "metadata": {
        "id": "aa21bANDf-uD"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def difference(x,y):\n",
        "  return tf.reduce_sum(tf.square(y-x))"
      ],
      "metadata": {
        "id": "y4QGgntPgfwZ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def yolo_loss(y_true, y_pred):\n",
        "  target = y_true[...,0]\n",
        "\n",
        "  ###################### objectness loss for object ########################\n",
        "  y_pred_extract = tf.gather_nd(y_pred, tf.where(target[:]==1))\n",
        "  y_target_extract = tf.gather_nd(y_true, tf.where(target[:]==1))\n",
        "\n",
        "  rescaler = tf.where(target[:]==1)*SPLIT_SIZE\n",
        "  upscaler_1 = tf.concat([rescaler[:,1:],tf.zeros([len(rescaler),2], dtype=tf.int64)],axis=-1)\n",
        "  # converting the grid level coordinates to pixel level coordinates for better understanding\n",
        "\n",
        "  target_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
        "                       repeats=[len(rescaler)], axis=0)*tf.cast(y_target_extract[...,1:5], dtype = tf.float32)\n",
        "  pred_1_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
        "                      repeats=[len(rescaler)], axis=0)*tf.cast(y_pred_extract[...,1:5], dtype = tf.float32)\n",
        "  pred_2_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
        "                      repeats=[len(rescaler)], axis=0)*tf.cast(y_pred_extract[...,6:10], dtype = tf.float32)\n",
        "\n",
        "  target_orig = tf.cast(upscaler_1, dtype = tf.float32)+target_upscaler_2\n",
        "  pred_1_orig = tf.cast(upscaler_1, dtype = tf.float32)+pred_1_upscaler_2\n",
        "  pred_2_orig = tf.cast(upscaler_1, dtype = tf.float32)+pred_2_upscaler_2\n",
        "\n",
        "  mask =tf.cast(tf.math.greater(compute_iou(target_orig,pred_2_orig),\n",
        "                                         compute_iou(target_orig,pred_1_orig)),dtype=tf.int32)\n",
        "\n",
        "  y_pred_joined=tf.transpose(tf.concat([tf.expand_dims(y_pred_extract[...,0],axis=0),\n",
        "                        tf.expand_dims(y_pred_extract[...,5],axis=0)],axis=0))\n",
        "\n",
        "  obj_pred = tf.gather_nd(y_pred_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
        "\n",
        "  object_loss = difference(tf.cast(obj_pred,dtype =tf.float32)\n",
        "                            ,tf.cast(tf.ones([len(rescaler)]),dtype=tf.float32))\n",
        "\n",
        "  ###################### objectness loss for no object ########################\n",
        "  y_pred_extract = tf.gather_nd(y_pred[...,0:B*5], tf.where(target[:]==0))\n",
        "  y_target_extract = tf.zeros(len(y_pred_extract))\n",
        "\n",
        "  no_object_loss_1 = difference(tf.cast(y_pred_extract[...,0],dtype =tf.float32)\n",
        "                            ,tf.cast(y_target_extract,dtype=tf.float32))\n",
        "\n",
        "  no_object_loss_2 = difference(tf.cast(y_pred_extract[...,5],dtype =tf.float32)\n",
        "                            ,tf.cast(y_target_extract,dtype=tf.float32))\n",
        "\n",
        "  no_object_loss = no_object_loss_1 + no_object_loss_2\n",
        "\n",
        "   ###################### class loss for object ########################\n",
        "  y_pred_extract = tf.gather_nd(y_pred[...,10:],tf.where(target[:]==1)) # prediction start from 10 because of 2 AnchorBoxes i.e. 2 lambdas\n",
        "  class_extract = tf.gather_nd(y_true[...,5:],tf.where(target[:]==1)) # True has just one lambdas i.e. 1 objectness score\n",
        "\n",
        "  class_loss = difference(tf.cast(y_pred_extract,dtype =tf.float32)\n",
        "                                ,tf.cast(class_extract,dtype=tf.float32))\n",
        "\n",
        "   ###################### Bounding Box loss for object ########################\n",
        "  y_pred_extract = tf.gather_nd(y_pred[...,0:B*5], tf.where(target[:]==1))\n",
        "  centre_joined=tf.stack([y_pred_extract[...,1:3],y_pred_extract[...,6:8]],axis=1)\n",
        "  centre_pred = tf.gather_nd(centre_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
        "  centre_target = tf.gather_nd(y_true[...,1:3], tf.where(target[:]==1))\n",
        "\n",
        "  centre_loss = difference(centre_pred,centre_target)\n",
        "\n",
        "  size_joined=tf.stack([y_pred_extract[...,3:5],y_pred_extract[...,8:10]],axis=1)\n",
        "\n",
        "  size_pred = tf.gather_nd(size_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
        "  size_target = tf.gather_nd(y_true[...,3:5], tf.where(target[:]==1))\n",
        "\n",
        "  size_loss = difference(tf.math.sqrt(tf.math.abs(size_pred)),tf.math.sqrt(tf.math.abs(size_target)))\n",
        "  box_loss = centre_loss+size_loss\n",
        "\n",
        "  lambda_coord = 5.0\n",
        "  lambda_no_obj = 0.5\n",
        "\n",
        "  loss = object_loss + (lambda_no_obj*no_object_loss)+ tf.cast(lambda_coord*box_loss,dtype=tf.float32)+ tf.cast(class_loss,dtype=tf.float32)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ZpLLWagdKExc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets explain the steps:\n",
        "- First the target position is learned\n",
        "- Then the location of the true position and predicted position is extracted,\n",
        "- After that the grid cell position is converted to pixel level coordinates done by the rescaler(helps in localization) and also adjusts the position of the predicted bb,\n",
        "- Then the upscaler_1 adjusts the attention just to the predicted parts where the objectness score is high and it makes the other parts of the image irrelevant(it finds the centre point of the object and provides it coordinates),\n",
        "- Target_Upscaler_2, Pred1 & 2_Upscaler_2: These operations help in finding the location of the bb and make sure that the bb are aligned with the grid cells and also make sure that the coordinates of true bb and predicted bb are in the same unit for comparison, they also are plotted in pixel values,\n",
        "- The pixel values are calculated in target_org, pred_1_org and pred_2_org,\n",
        "- Then the IOU score is computed using the mask operation where the IOU is compared to the ground truth and seen which is closer, tf.greater returns boolean value which says where the pred1 or pred2 is closer, then using tf.cast the boolean is converted to 0 or 1 given true or false and this relays the position of the pred and allots which one is closer\n",
        "- Then the y_pred_joined is used to extract the lambda values which is the objectness score of each bb and create a tensor where each score is placed one above the other for further processing,\n",
        "- obj_pred basically take only the one with the mask which is the one with higher IOU score out of the lambdas(2 bb's) and stores it,\n",
        "- Then the objectness loss is calculated for example, if the objectness score is 0.9 then its 1 - 0.9 which is 0.1\n",
        "- Next is phase 2 of calculating loss where there are no object, the formula for objectness since there is no object is (0 - lambda1)^2 + (0 - lambda2)^2 no_object_loss_1 has 0 index and no_object_loss_2 has 5 index then we add it up to compute total no object loss\n",
        "- Anchor boxes or B here in this context is basically a preset number of boxes that will be associated with each grid cell, normally one is assigned but with increase in number the shape changes for example for one box its size, size, classes + 5 * Box size(1 in this case), with every inference these anchor or guide boxes are adjusted with respect to the ground truth,\n",
        "- 3rd Phase we obtain the class loss by just focusing on the grids that contain the objects,\n",
        "- 4th Phase we obtain the bounding box loss where we calculate for only where the object is present, we then extract the x, y coordinates from both the anchor boxes based on index 1:3 , 6:8 since there are two in centre_joined, then we only take the masked box which is the one with higher IOU value and store it in centre_pred, then we calculate the centre loss by using difference func on centre_pred and centre_true,\n",
        "- Then we stack up the width and height in size_pred and same process and eventually we calculate the size loss\n",
        "- Then we add up both the losses to get box_loss,\n",
        "- The Final Phase is total loss calculation, here we add up all the losses and use lambda obj and lambda no object to calculate the summation of the losses. Basically there were two lambdas where one was used for object detection and the other was used for background detection,i.e. No object detection. These twp lambdas are basically penalizing techniques to make the model predict better"
      ],
      "metadata": {
        "id": "KfJ7a7DhWAdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Compilation and Fitting"
      ],
      "metadata": {
        "id": "VC7sJQHRGAhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=yolo_loss,\n",
        "    optimizer=Adam(1e-3)\n",
        ")"
      ],
      "metadata": {
        "id": "UbXRovnIJXK0"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs = N_EPOCHS, verbose = 1, callbacks = [callback, lr_callback] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06GXnLX7FSgj",
        "outputId": "83dc351c-8afa-4819-c06c-d8e9ae4b4d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "534/534 [==============================] - 210s 342ms/step - loss: 189.2010 - val_loss: 181.6989 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "534/534 [==============================] - 137s 256ms/step - loss: 152.6757 - val_loss: 169.7345 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "534/534 [==============================] - 136s 254ms/step - loss: 141.3712 - val_loss: 164.5220 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "534/534 [==============================] - 135s 253ms/step - loss: 132.0849 - val_loss: 154.3519 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "534/534 [==============================] - 137s 257ms/step - loss: 124.5069 - val_loss: 152.9073 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "534/534 [==============================] - 135s 253ms/step - loss: 117.3682 - val_loss: 151.8862 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "534/534 [==============================] - 135s 252ms/step - loss: 111.4462 - val_loss: 148.6520 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "534/534 [==============================] - 133s 249ms/step - loss: 105.7168 - val_loss: 150.0228 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "502/534 [===========================>..] - ETA: 7s - loss: 101.2857"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "x5vJdtVqGQQF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSGD2v-YGS2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}